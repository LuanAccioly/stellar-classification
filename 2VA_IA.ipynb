{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**PRÉ-PROCESSAMENTO**\n",
        "\n",
        "1. **Ao menos um método de pré-processamento deve ser usado**, gerando uma nova base de dados.\n",
        "\n",
        "1. O tipo de pré-processamento utilizado deve estar relacionado ao **contexto da aplicação**.\n",
        "\n",
        "1. Remoção de vírgulas, espaços em branco, identificador dos padrões, etc. **não serão considerados pré-processamento válidos**.\n"
      ],
      "metadata": {
        "id": "GKzXHM5u-_JA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**EXPERIMENTOS E ANÁLISE DOS RESULTADOS**\n",
        "\n",
        "1. Os experimentos devem ser executados de acordo com o esquema abaixo para cada uma das bases de dados geradas (tanto a base de dados “brutos” quanto a base de dados pré-processadas):\n",
        "   1. **Deve-se executar o 10-*fold cross-validation* 5 vezes para cada base de dados**, com cada uma das cinco execuções partindo de uma distribuição aleatória dos dados entre cada *fold*, resultando em um total de 50 experimentos por base de dados (10 x 5).\n",
        "   1. Em cada um dos 50 experimentos, **os conjuntos de treinamento e teste devem ser mantido o mesmo para cada algoritmo a ser testado** (mesmo ponto de partida para cada modelo), de modo a obter-se uma avaliação justa dos resultados.\n",
        "   1. Ao menos **três algoritmos** devem ser testados e comparados:\n",
        "      1. **Árvore de Decisão;**\n",
        "      1. **Naïve Bayes;**\n",
        "      1. **K-Vizinhos Mais Próximos (K-NN) -** variando-se **3 vezes o número do parâmetro *k*;**\n",
        "      1. **Rede Neural Artificial treinada por *Backpropagation***.\n",
        "      1. **Outros Algoritmos de Aprendizagem Supervisionada (Classificadores) mediante validação prévia do Professor**.\n",
        "1. Ao menos **duas métricas (índices) de avaliação** deverão ser empregadas na análise experimental, **além do tempo médio de execução de cada um dos algoritmos**.\n",
        "1. As métricas escolhidas devem ser justificadas pela **Revisão da Literatura**.\n",
        "\n",
        "A **análise experimental** deve ser feita de forma **empírica** (baseado nas medidas obtidas) e **através de uma discussão dos resultados experimentais**.\n"
      ],
      "metadata": {
        "id": "_b94WyQN_DwM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 98,
      "metadata": {
        "id": "yuW7hIe-T3RD"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import time\n",
        "import numpy as np\n",
        "from google.colab import files\n",
        "from pathlib import Path\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn import tree\n",
        "from sklearn.model_selection import cross_val_score, KFold, cross_validate"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def time_converter(seconds):\n",
        "    hours = seconds // 3600\n",
        "    minutes = (seconds % 3600) // 60\n",
        "    seconds = seconds % 60\n",
        "\n",
        "    return f\"{hours:02}h {minutes:02}m {seconds:02}s\"\n",
        "\n",
        "# Exemplo de uso:\n",
        "tempo_em_segundos = 60\n",
        "tempo_formatado = time_converter(tempo_em_segundos)\n",
        "\n",
        "print(f\"Tempo formatado: {tempo_formatado}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KXn8qcyZfcPd",
        "outputId": "0b2a9d8a-36fe-4123-a882-6345985dee3e"
      },
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tempo formatado: 00h 01m 00s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Transforma:\n",
        "# [\n",
        "#  {'accuracy': [], 'precision_weighted': []},\n",
        "#  {'accuracy': [], 'precision_weighted': []}\n",
        "# ]\n",
        "#\n",
        "# em :\n",
        "#\n",
        "# {\n",
        "#  'accuracy': [[], []],\n",
        "#  'precision_weighted': [[], []]\n",
        "# }\n",
        "\n",
        "def process_scores(data):\n",
        "    processed_data = {}\n",
        "    for metric_dict in data:\n",
        "        for metric, values in metric_dict.items():\n",
        "            if metric not in processed_data:\n",
        "                processed_data[metric] = []\n",
        "            processed_data[metric].append(values)\n",
        "    return processed_data"
      ],
      "metadata": {
        "id": "G1LE0y-Yg0sJ"
      },
      "execution_count": 100,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if not Path('/content/star_classification.csv').exists():\n",
        "  _ = files.upload()\n",
        "df = pd.read_csv('star_classification.csv')\n",
        "\n",
        "classes_count = df['class'].value_counts()\n",
        "\n",
        "print(classes_count)"
      ],
      "metadata": {
        "id": "Stb3NOAMXfsZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "093c2435-24fc-47f3-f965-8c34337e347c"
      },
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GALAXY    59445\n",
            "STAR      21594\n",
            "QSO       18961\n",
            "Name: class, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Separando 20 linhas de cada classe\n",
        "\n",
        "galaxy_subset = df[df['class'] == 'GALAXY']\n",
        "star_subset = df[df['class'] == 'STAR']\n",
        "qso_subset = df[df['class'] == 'QSO']\n",
        "\n",
        "galaxy_prediction = galaxy_subset.sample(n=20)\n",
        "star_prediction = star_subset.sample(n=20)\n",
        "qso_prediction = qso_subset.sample(n=20)\n",
        "\n",
        "\n",
        "# Excluindo da base original as linhas retiradas para predição\n",
        "prediction_df = pd.concat([galaxy_prediction, star_prediction, qso_prediction])\n",
        "brute_df = df.drop(prediction_df.index)\n",
        "\n",
        "# Verificando se a base para predição ainda está dentro da base de treino\n",
        "contains = prediction_df.isin(brute_df).all().all()\n",
        "\n",
        "print(\"A base para treino contém dados da base de predição\"\n",
        "      if contains\n",
        "      else \"A base para treino está limpa\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pZNERnQG-tPn",
        "outputId": "c0cc47f8-1c1f-4d1d-e533-5d11f61ad608"
      },
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "A base para treino está limpa\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "brute_df.columns"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RVDie9SiYsTD",
        "outputId": "fd56b556-3e71-47d8-ce05-cebbe502085b"
      },
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['obj_ID', 'alpha', 'delta', 'u', 'g', 'r', 'i', 'z', 'run_ID',\n",
              "       'rerun_ID', 'cam_col', 'field_ID', 'spec_obj_ID', 'class', 'redshift',\n",
              "       'plate', 'MJD', 'fiber_ID'],\n",
              "      dtype='object')"
            ]
          },
          "metadata": {},
          "execution_count": 103
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Separando as bases\n",
        "\n",
        "# Base sem processamento\n",
        "brute_X = brute_df.drop(columns=['class']).to_numpy()\n",
        "\n",
        "# Processamento\n",
        "# Removendo colunas de ID\n",
        "processed_df = brute_df.drop(columns=['obj_ID',\n",
        "                                      'field_ID',\n",
        "                                      'spec_obj_ID',\n",
        "                                      'fiber_ID',\n",
        "                                      'plate',\n",
        "                                      'run_ID',\n",
        "                                      'rerun_ID',\n",
        "                                      'MJD'])\n",
        "\n",
        "preprocessed_X, y = processed_df.drop(columns=['class']).to_numpy(), brute_df['class'].to_numpy()\n",
        "\n",
        "# Normalizando valores. -1 : 1\n",
        "scaler = MinMaxScaler(feature_range=(-1, 1))\n",
        "processed_X = scaler.fit_transform(preprocessed_X)"
      ],
      "metadata": {
        "id": "keA69VzpYvJ0"
      },
      "execution_count": 104,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Base bruta:')\n",
        "display(brute_X)\n",
        "\n",
        "print('\\n\\nBase processada:')\n",
        "display(processed_X)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 568
        },
        "id": "xm4d3v6Ijbu2",
        "outputId": "d6ab5552-e123-4a91-8c1f-ebc4d3231247"
      },
      "execution_count": 105,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Base bruta:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "array([[1.23766096e+18, 1.35689107e+02, 3.24946318e+01, ...,\n",
              "        5.81200000e+03, 5.63540000e+04, 1.71000000e+02],\n",
              "       [1.23766488e+18, 1.44826101e+02, 3.12741849e+01, ...,\n",
              "        1.04450000e+04, 5.81580000e+04, 4.27000000e+02],\n",
              "       [1.23766096e+18, 1.42188790e+02, 3.55824442e+01, ...,\n",
              "        4.57600000e+03, 5.55920000e+04, 2.99000000e+02],\n",
              "       ...,\n",
              "       [1.23766830e+18, 2.24587407e+02, 1.57007074e+01, ...,\n",
              "        2.76400000e+03, 5.45350000e+04, 7.40000000e+01],\n",
              "       [1.23766115e+18, 2.12268621e+02, 4.66603653e+01, ...,\n",
              "        6.75100000e+03, 5.63680000e+04, 4.70000000e+02],\n",
              "       [1.23766115e+18, 1.96896053e+02, 4.94646428e+01, ...,\n",
              "        7.41000000e+03, 5.71040000e+04, 8.51000000e+02]])"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "Base processada:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "array([[-0.24619037,  0.00760492,  0.99822513, ...,  0.99788799,\n",
              "        -0.6       , -0.81633828],\n",
              "       [-0.19542848, -0.01637576,  0.99840431, ...,  0.9984505 ,\n",
              "         0.6       , -0.77522221],\n",
              "       [-0.21008044,  0.06827764,  0.9985011 , ...,  0.99791881,\n",
              "        -0.6       , -0.81366043],\n",
              "       ...,\n",
              "       [ 0.24769693, -0.32238054,  0.99768491, ...,  0.9976005 ,\n",
              "         0.2       , -0.95632202],\n",
              "       [ 0.17925814,  0.28594879,  0.99851851, ...,  0.99785431,\n",
              "         0.2       , -0.86754138],\n",
              "       [ 0.09385362,  0.34105031,  0.9979745 , ...,  0.99798512,\n",
              "         0.2       , -0.84250167]])"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Instanciando modelos\n",
        "rf = RandomForestClassifier()\n",
        "dt = tree.DecisionTreeClassifier()\n",
        "nb = GaussianNB()\n",
        "knn = KNeighborsClassifier()"
      ],
      "metadata": {
        "id": "2PXDpZxvZa0N"
      },
      "execution_count": 106,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_results_table(algorithms, scores, times):\n",
        "    total_time = [round(sum(fold_times), 3) for fold_times in times]\n",
        "    avg_seed_time = [round(np.mean(fold_times), 3) for fold_times in times]\n",
        "    df_resultados = pd.DataFrame({\n",
        "        'Algoritmo': algorithms,\n",
        "        'Acurácia média': scores['accuracy'],\n",
        "        'Precisão média': scores['precision_weighted'],\n",
        "        'Recall médio': scores['recall_weighted'],\n",
        "        'F1 médio': scores['f1_weighted'],\n",
        "        'Tempo médio por seed': avg_seed_time,\n",
        "        'Tempo total': total_time\n",
        "    })\n",
        "\n",
        "    return df_resultados"
      ],
      "metadata": {
        "id": "FaunJLCKYECS"
      },
      "execution_count": 107,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_algorithm_comparison(algorithms_names,\n",
        "                              databases,\n",
        "                              inter_bases_scores_values,\n",
        "                              scoring_name):\n",
        "    bar_width = 0.35\n",
        "    index = np.arange(len(algorithms_names))\n",
        "\n",
        "    plt.figure(figsize=(12, 6))\n",
        "\n",
        "    # Loop para cada par de colunas (bruto e processado)\n",
        "    for i, scores in enumerate(inter_bases_scores_values):\n",
        "        bars = plt.bar(index + i * bar_width, scores, bar_width, label=f'{databases[i]}')\n",
        "        for bar, score in zip(bars, scores):\n",
        "            plt.text(bar.get_x() + bar.get_width() / 2,\n",
        "                     bar.get_height() / 2,\n",
        "                     f'{score:.3f}',\n",
        "                     ha='center',\n",
        "                     va='center',\n",
        "                     fontsize='x-large',\n",
        "                     fontweight='bold',\n",
        "                     color='white'\n",
        "                     )\n",
        "\n",
        "    plt.xlabel('Algoritmos')\n",
        "    plt.ylabel(scoring_name)\n",
        "    plt.title(scoring_name.upper())\n",
        "    plt.xticks(index + (len(databases) - 1) * bar_width / 2, algorithms_names)\n",
        "    plt.legend()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "jc8G-1IBtdZS"
      },
      "execution_count": 108,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "seeds = [2, 4, 8, 16, 32]\n",
        "folds=10\n",
        "algorithms = [rf, nb, knn, dt]\n",
        "databases = [brute_X, processed_X]\n",
        "inter_bases_scores_values = []\n",
        "\n",
        "algorithms_names = ['Random Forest',\n",
        "                    'NaiveBayes',\n",
        "                    'Knn',\n",
        "                    'Decision Tree']\n",
        "\n",
        "databases_names = ['Base bruta',\n",
        "                   'Base processada']\n",
        "\n",
        "scoring_names = ['accuracy',\n",
        "                 'precision_weighted',\n",
        "                 'recall_weighted',\n",
        "                 'f1_weighted']\n",
        "\n",
        "\n",
        "# Para cada base de dados (brute_X, processed_X)\n",
        "for index, X in enumerate(databases):\n",
        "  score_values = {'accuracy': [],\n",
        "                  'precision_weighted': [],\n",
        "                  'recall_weighted': [],\n",
        "                  'f1_weighted': []}\n",
        "  time_values = []\n",
        "  total_time_values = []\n",
        "\n",
        "  start_database_time = time.time()\n",
        "\n",
        "  # Ao menos três algoritmos devem ser testados e comparados\n",
        "  for algorithm in algorithms:\n",
        "    algorithm_times = []\n",
        "    algorithm_scores = {'accuracy': [],\n",
        "                        'precision_weighted': [],\n",
        "                        'recall_weighted': [],\n",
        "                        'f1_weighted': []}\n",
        "\n",
        "    start_algorithm_time = time.time()\n",
        "\n",
        "    # Cinco execuções partindo de uma distribuição aleatória\n",
        "    # dos dados entre cada fold\n",
        "    for seed in seeds:\n",
        "      knn.n_neighbors = seed\n",
        "\n",
        "      # Definindo a aleatoriedade dos folds\n",
        "      kf = KFold(n_splits=folds,\n",
        "                 shuffle=True,\n",
        "                 random_state=seed)\n",
        "\n",
        "      start_seed_time = time.time()\n",
        "\n",
        "      scores = cross_validate(algorithm,\n",
        "                              X,\n",
        "                              y,\n",
        "                              cv=kf,\n",
        "                              scoring=scoring_names)\n",
        "\n",
        "      final_seed_time = time.time()\n",
        "\n",
        "      algorithm_times.append(final_seed_time - start_seed_time)\n",
        "\n",
        "      # Populando o dicionário dos scores a nível de seed\n",
        "      for key in algorithm_scores:\n",
        "                algorithm_scores[key].append(scores['test_' + key].mean())\n",
        "\n",
        "    final_algorithm_time = time.time()\n",
        "\n",
        "    total_time_values.append(final_algorithm_time - start_algorithm_time)\n",
        "    time_values.append(algorithm_times)\n",
        "\n",
        "    # Populando o dicinário dos scores a nível de algoritmo\n",
        "    for key in score_values:\n",
        "            score_values[key].append(np.mean(algorithm_scores[key]))\n",
        "\n",
        "  final_databse_time = time.time()\n",
        "\n",
        "  # Definindo os scores para a base de dados em execução\n",
        "  inter_bases_scores_values.append(score_values)\n",
        "\n",
        "\n",
        "  print('\\n\\n{}'.format(databases_names[index]))\n",
        "  print('Tempo total: {}'.format((final_databse_time - start_database_time) / 60))\n",
        "  display(generate_results_table(algorithms_names,  score_values, time_values))\n"
      ],
      "metadata": {
        "id": "Xqq0iVc0LEFZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "processed_scores = process_scores(inter_bases_scores_values)\n",
        "\n",
        "for score in scoring_names:\n",
        "\n",
        "  print('\\n\\n')\n",
        "  plot_algorithm_comparison(algorithms_names,\n",
        "                            databases_names,\n",
        "                            processed_scores[score],\n",
        "                            score)"
      ],
      "metadata": {
        "id": "mq92Xc-vITTc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}